import streamlit as st
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForCausalLM
import re
from gtts import gTTS
import io

# helper: fading function ---
def get_faded_prompt(words, fade_level):
    """Return the narrative with the last `fade_level` words replaced by blanks."""
    if fade_level == 0:
        return " ".join(words)
    faded_words = [
        word if i < len(words) - fade_level else "___"
        for i, word in enumerate(words)
    ]
    return " ".join(faded_words)

# app UI 
# Page configuration (sets favicon, title, and center page)
st.set_page_config(
    page_title="Image-to-Narrative",
    layout="centered"
)

st.markdown("""
    <div style='text-align: center; padding: 2rem 0;'>
        <h1 style='color: #9611B3;'>Welcome to the Image Narrator ðŸ‘‹</h1>
        <p style='font-size: 1.2rem;'>Upload your image and get a friendly, narrated description!</p>
    </div>
""", unsafe_allow_html=True)

# tooltips & help 
with st.expander("What is this app?"):
    st.write("""
    This app helps you describe images in a simple way. 
    You can upload a photo, get a short narrative, and practice filling in the blanks as words are faded out.
    Great for conversation practice and social support!
    """)

# user uploads image 
uploaded_file = st.file_uploader(
    "Choose an image file (jpg, jpeg, png)...", 
    type=["jpg", "jpeg", "png"],
    help="Upload a clear photo (jpg, jpeg, png). Max size: 5MB."
)

# load model and processor (cache to avoid reloading) 
@st.cache_resource
def load_model():
    model_id = "visheratin/MC-LLaVA-3b"
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True
    ).to("cuda" if torch.cuda.is_available() else "cpu")
    return processor, model

processor, model = load_model()

# cleaner response code for TTS
def clean_response(text, prompt):
    # Remove prompt and special tokens from the generated output
    cleaned = text.replace(prompt, "")
    cleaned = re.sub(r"<\|.*?\|>", "", cleaned)
    return cleaned.strip()

if uploaded_file is not None: # open and display the uploaded image
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Your uploaded image", use_container_width=True)
    
    # caption input with tooltip
    caption = st.text_input(
        "Optional: Add your own caption for this image",
        "",
        help="Add a short description, or leave blank for automatic caption."
    )
    if not caption:
        caption = " "  # Avoid empty caption in prompt

    # build/edit prompt
    prompt = (
    "<|im_start|>user\n"
    "<image>\n"
    "Describe this image with a single first-person sentence.\n"
    "Make it short and simple, no more than 10 words.\n"
    "Use first-person perspective (e.g., 'I see...', 'I feel...').\n"
    f"Caption: {caption}\n"
    "Narrative:\n"
    "<|im_end|>\n"
    "<|im_start|>assistant\n"
        )
    # preparing inputs for the model
    inputs = processor(prompt, [image], model, return_tensors="pt")
    inputs = {k: v.to(model.device) if torch.is_tensor(v) else v for k, v in inputs.items()}
    # Generate narrative
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=64,
            use_cache=True,
            do_sample=False,
            eos_token_id=processor.tokenizer.eos_token_id,
            pad_token_id=processor.tokenizer.eos_token_id
        )
    generated_text = processor.tokenizer.decode(output[0], skip_special_tokens=True)
    
    # Extract only the generated narrative
    narrative_parts = re.split(r"<\|im_start\|>assistant", generated_text)
    narrative = narrative_parts[-1] if len(narrative_parts) > 1 else generated_text
    narrative = narrative.replace("<|im_end|>", "").strip()
    words = narrative.split()
    
    # state for Fade Level
    if "fade_level" not in st.session_state:
        st.session_state.fade_level = 0
    max_fade = len(words)

    # buttons for fading
    col1, col2, col3 = st.columns([1,2,1])
    with col1:
        if st.button("Fade Less", help="Show more of the prompt (remove one blank)"):
            if st.session_state.fade_level > 0:
                st.session_state.fade_level -= 1
    with col3:
        if st.button("Fade More", help="Fade one more word from the end"):
            if st.session_state.fade_level < max_fade:
                st.session_state.fade_level += 1

    faded_prompt = get_faded_prompt(words, st.session_state.fade_level)
    st.markdown("**Your Practice Prompt:**")
    st.success(faded_prompt)
    
    # TTS Button 
    if st.button("ðŸ”Š Read Aloud"):
        tts = gTTS(narrative)
        mp3_fp = io.BytesIO()
        tts.write_to_fp(mp3_fp)
        mp3_fp.seek(0)
        st.audio(mp3_fp, format="audio/mp3")

    # optional, let a supporter/teacher reveal the full narrative if needed
    with st.expander("Show full narrative (for supporter/teacher use)"):
        st.info(narrative)
else:
    st.info("Please upload a jpg, jpeg, or png image to get started.")

st.markdown("---")
st.caption("Powered by MC-LLaVA-3b and Streamlit. For best results, use clear photos with obvious subjects!")