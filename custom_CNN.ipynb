{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ba9837-e2aa-4c00-b773-559ece7cfc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd96e8b-6f40-41ed-b9d7-6ea43cf3a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 (standard for ResNet)\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(           # Normalize using ImageNet mean and std\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ec720a-4a22-47ab-8748-c83a994f6480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 3440\n",
      "Class names: ['cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"cats_dogs_dataset/images/train\"  # Adjust if needed\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "print(f\"Number of images: {len(dataset)}\")\n",
    "print(f\"Class names: {dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b59eb21-279a-4fce-af61-4433c6a9c266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([32])\n",
      "Labels: tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader  # Import PyTorch utilities for splitting and loading data\n",
    "\n",
    "# Set the batch size for training and validation\n",
    "batch_size = 32  # You can adjust this based on your GPU/CPU memory\n",
    "\n",
    "# Calculate the number of images for training (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "val_size = len(dataset) - train_size  # The rest for validation\n",
    "\n",
    "# Randomly split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "# - batch_size: how many samples per batch to load\n",
    "# - shuffle=True: shuffle the data at every epoch for better training\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "# - shuffle=False: do not shuffle validation data, ensures consistent evaluation\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# (Optional) Inspect a batch from the training DataLoader to verify shapes and labels\n",
    "images, labels = next(iter(train_loader))  # Get a single batch\n",
    "print(f\"Image batch shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n",
    "print(f\"Labels batch shape: {labels.shape}\")  # Should be [batch_size]\n",
    "print(f\"Labels: {labels}\")  # Tensor of class indices (0 for cats, 1 for dogs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641e9494-5203-4feb-8c77-55e40c002279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0460\n",
      "Epoch [2/5], Loss: 0.6833\n",
      "Epoch [3/5], Loss: 0.6455\n",
      "Epoch [4/5], Loss: 0.5160\n",
      "Epoch [5/5], Loss: 0.3131\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer: input channels=3 (RGB), output channels=16, kernel size=3x3\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer with 2x2 window\n",
    "        # Second convolutional layer: input channels=16, output channels=32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        # Fully connected layer: input features=32*56*56, output features=64\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 64)\n",
    "        # Output layer: input features=64, output features=num_classes (cats and dogs)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through first conv, relu, and pool\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        # Pass through second conv, relu, and pool\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through first fully connected layer and relu\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # Output layer (no activation; handled by loss function)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. Instantiate the model and move it to the device (GPU or CPU)\n",
    "model = SimpleCNN(num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# 4. Training loop (one epoch as an example)\n",
    "num_epochs = 5  # You can increase this for better results\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        optimizer.zero_grad()             # Zero the parameter gradients\n",
    "        outputs = model(images)           # Forward pass\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "        loss.backward()                   # Backward pass\n",
    "        optimizer.step()                  # Update weights\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c1cb52-4bea-48de-a9c9-5871e1b81ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5601, Accuracy: 71.22%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878df25-9049-4218-859e-267e471cef1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
