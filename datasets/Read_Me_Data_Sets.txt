Here are the links to the datasets used

1. **Flickr30k**  
   - Contains the Flickr30k dataset: approximately 31,783 images collected from Flickr, each annotated with five human-written captions. It’s a widely-used benchmark for image captioning and vision-language research :contentReference[oaicite:1]{index=1}.
   -Link: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset

2. **EgoCap (First‑Person Captioning Dataset)**  
   - Contains the EgoCap dataset (also referred to as the First‑Person Captioning Dataset): comprised of around 2,100 egocentric (first-person view) images, over 10,000 first-person narrative captions, and 6,300 contextual labels. It is specifically designed for egocentric, or life‑logging, captioning research and introduces challenges such as noisy viewpoints and perspective-aware semantics.
   -Link: https://github.com/zdai257/EgoCap-EgoFormer/tree/main/datasets

