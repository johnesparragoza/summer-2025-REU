import os
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from datasets import load_dataset, Image as HfImage
import matplotlib.pyplot as plt
from accelerate import infer_auto_device_map
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from PIL import Image as PILImage

# ==== CONFIGURATION ====
BATCH_SIZE = 2
EPOCHS = 5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN = 64
IMAGE_DIR = "Images_and_Captions/Images"
CAPTIONS_FILE = "Images_and_Captions/training_captions.txt"

# ==== SPLIT DATA 80/20 ====
with open(CAPTIONS_FILE, "r") as f:
    lines = [line.strip() for line in f if line.strip()]

train_lines, val_lines = train_test_split(lines, test_size=0.2, random_state=42, shuffle=True)

os.makedirs("Images_and_Captions/tmp_split", exist_ok=True)
train_file = "Images_and_Captions/tmp_split/train.txt"
val_file = "Images_and_Captions/tmp_split/val.txt"
with open(train_file, "w") as f:
    f.write("\n".join(train_lines))
with open(val_file, "w") as f:
    f.write("\n".join(val_lines))

# ==== LOAD DATASET ====
dataset = load_dataset("text", data_files={"train": train_file, "validation": val_file})

# ==== SPLIT LINES INTO IMAGE AND CAPTION ====
def split_line(example):
    image_name, caption = example["text"].split(",", 1)
    full_image_path = os.path.join(IMAGE_DIR, image_name.strip())
    return {"image": full_image_path, "caption": caption.strip()}

def check_image(example):
    if not os.path.exists(example["image"]):
        raise FileNotFoundError(f"Missing file: {example['image']}")
    return example

dataset = dataset.map(split_line)
dataset = dataset.map(check_image)
dataset = dataset.cast_column("image", HfImage())

# ==== LOAD PROCESSOR AND MODEL ====
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b", use_fast=True)

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    device_map=None,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
)

device_map = infer_auto_device_map(model, no_split_module_classes=["OPTDecoderLayer"])

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    device_map=device_map,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
)

# ==== BATCH PREPROCESS FUNCTION ====
def preprocess(batch):
    images = batch["image"]
    captions = batch["caption"]

    for i, img in enumerate(images):
        if not isinstance(img, PILImage.Image):
            images[i] = PILImage.open(img).convert("RGB")

    inputs = processor(
        images=images,
        text=captions,
        truncation=True,
        max_length=MAX_LEN,
        padding=True,  # <-- MUST pad here for batches
        return_tensors="pt",
    )

    labels = inputs.input_ids.clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100

    return {
        "pixel_values": inputs.pixel_values,
        "input_ids": inputs.input_ids,
        "attention_mask": inputs.attention_mask,
        "labels": labels,
    }

# ==== APPLY PREPROCESS WITH BATCHING ====
train_dataset = dataset["train"].map(
    preprocess,
    remove_columns=["image", "caption", "text"],
    batched=True,
    batch_size=100,
    num_proc=1,  # increase if you want parallelism and enough RAM/disk space
)

val_dataset = dataset["validation"].map(
    preprocess,
    remove_columns=["image", "caption", "text"],
    batched=True,
    batch_size=100,
    num_proc=1,
)

train_dataset.set_format(type="torch")
val_dataset.set_format(type="torch")

# ==== PYTORCH WRAPPER ====
class HuggingfaceDatasetTorch(Dataset):
    def __init__(self, hf_dataset):
        self.dataset = hf_dataset

    def __getitem__(self, idx):
        return self.dataset[idx]

    def __len__(self):
        return len(self.dataset)

train_dataset_torch = HuggingfaceDatasetTorch(train_dataset)
val_dataset_torch = HuggingfaceDatasetTorch(val_dataset)

# ==== COLLATE FUNCTION WITH MANUAL LABEL PADDING ====
def collate_fn(batch):
    pixel_values = torch.stack([item["pixel_values"] for item in batch])

    input_ids = [item["input_ids"] for item in batch]
    attention_mask = [item["attention_mask"] for item in batch]
    labels = [item["labels"] for item in batch]

    batch_encoding = processor.tokenizer.pad(
        {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        },
        padding=True,
        return_tensors="pt",
    )

    max_length = batch_encoding["input_ids"].shape[1]

    padded_labels = torch.full((len(labels), max_length), fill_value=-100, dtype=torch.long)
    for i, label in enumerate(labels):
        length = label.size(0)
        padded_labels[i, :length] = label

    return {
        "pixel_values": pixel_values,
        "input_ids": batch_encoding["input_ids"],
        "attention_mask": batch_encoding["attention_mask"],
        "labels": padded_labels,
    }

# ==== DATALOADERS ====
train_loader = DataLoader(train_dataset_torch, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset_torch, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

# ==== OPTIMIZER ====
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# ==== TRAIN/VAL LOOP ====
train_losses, val_losses, val_accuracies = [], [], []

def compute_accuracy(preds, labels):
    return sum([p == l for p, l in zip(preds, labels)]) / len(preds)

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} - Training"):
        batch = {k: v.to(DEVICE) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    model.eval()
    val_loss = 0.0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} - Validation"):
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            val_loss += loss.item()
            generated_ids = model.generate(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_new_tokens=MAX_LEN,
            )
            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)
            labels = processor.batch_decode(batch["labels"], skip_special_tokens=True)
            all_preds.extend(preds)
            all_labels.extend(labels)
    avg_val_loss = val_loss / len(val_loader)
    val_losses.append(avg_val_loss)
    val_acc = compute_accuracy(all_preds, all_labels)
    val_accuracies.append(val_acc)
    print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}")

# ==== PLOT ====
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss Curves")

plt.subplot(1, 2, 2)
plt.plot(val_accuracies, label="Val Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Validation Accuracy")
plt.tight_layout()
plt.show()

# ==== FINAL SAMPLE EVAL ====
model.eval()
sample_raw = dataset["validation"][0]
sample_image = sample_raw["image"]
sample_caption = sample_raw["caption"]
with torch.no_grad():
    inputs = processor(images=sample_image, return_tensors="pt").to(DEVICE)
    generated_ids = model.generate(**inputs, max_new_tokens=MAX_LEN)
    caption = processor.decode(generated_ids[0], skip_special_tokens=True)
print("Sample image:", sample_image)
print("Ground truth caption:", sample_caption)
print("Model prediction:", caption)
