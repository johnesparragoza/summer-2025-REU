{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99be8d1-a5a7-43b6-bef4-83e205ba6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear Python garbage\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally reset CUDA memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918f3828-876a-4c3c-a7a8-7f5df2a0bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in ./anaconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./anaconda3/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in ./anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./anaconda3/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate\n",
    "!pip install transformers datasets accelerate peft torchvision torchaudio opencv-python\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    AutoProcessor, LlavaForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454edff7-5b51-4cd5-b53f-a047a12d3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1362cd0c342a4963372f19dcadd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BLIP setup for caption generation\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# LLaVA setup for grounded narrative\n",
    "llava_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "llava_processor = AutoProcessor.from_pretrained(llava_id)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    llava_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faa914cc-e1aa-450c-9f41-143d4edc6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hybrid_narrative(image_path, context=\"\"):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Step 1: BLIP caption\n",
    "    blip_inputs = blip_processor(images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    blip_ids = blip_model.generate(**blip_inputs, max_new_tokens=30)\n",
    "    caption = blip_processor.decode(blip_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Optional: Let user override the caption\n",
    "    print(\"Auto-caption:\", caption)\n",
    "    usr = input(\"Edit caption or press Enter to keep: \").strip()\n",
    "    if usr:\n",
    "        caption = usr\n",
    "\n",
    "    # Step 2: LLaVA narrative using caption + context\n",
    "    prompt = f\"\"\"<image>\\nDescribe this image in first person using short and simple sentences.\n",
    "Caption: \"{caption}\"\n",
    "Context: {context}\n",
    "\n",
    "\"\"\"\n",
    "    llava_inputs = llava_processor(prompt, image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    llava_ids = llava_model.generate(\n",
    "        **llava_inputs,\n",
    "        max_new_tokens=80,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=llava_processor.tokenizer.eos_token_id\n",
    "    )\n",
    "    narrative = llava_processor.decode(llava_ids[0], skip_special_tokens=True)\n",
    "    return { \"caption\": caption, \"narrative\": narrative }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80dda8fc-9b8c-406a-a01f-ed6bcbb0b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-caption: man holding a baby\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Edit caption or press Enter to keep:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Image: 4858206982.jpg\n",
      "Original Caption: A sitting man holding a baby .\n",
      "\n",
      "Generated Narrative:\n",
      "\n",
      "Describe this image in first person using short and simple sentences.\n",
      "Caption: \"man holding a baby\"\n",
      "Context: This is something I saw today.\n",
      "\n",
      "I am holding a baby.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the CSV with image names and captions\n",
    "csv_path = \"/home/hipe2/Pictures/30k_dataset/flickr30k_images/results.csv\"\n",
    "image_dir = \"/home/hipe2/Pictures/30k_dataset/flickr30k_images/images_\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path, delimiter='|')\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "\n",
    "# Randomly select a row\n",
    "random_row = df.sample(n=1).iloc[0]\n",
    "selected_image = random_row['image'].strip()\n",
    "caption = random_row['caption'].strip()\n",
    "\n",
    "# Full image path\n",
    "image_path = os.path.join(image_dir, selected_image)\n",
    "\n",
    "# Optional: You could ask the user for extra context input here\n",
    "context = \"This is something I saw today.\"\n",
    "\n",
    "# Run the narrative generation\n",
    "result = generate_hybrid_narrative(image_path, context)\n",
    "\n",
    "# Output\n",
    "print(f\"Selected Image: {selected_image}\")\n",
    "print(f\"Original Caption: {caption}\")\n",
    "print(\"\\nGenerated Narrative:\")\n",
    "print(result['narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78749f7d-cb26-438a-b667-8c4339a21e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-caption: a group of people standing in a room\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Edit caption or press Enter to keep:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Narrative:\n",
      " \n",
      "Describe this image in first person using short and simple sentences.\n",
      "Caption: \"a group of people standing in a room\"\n",
      "Context: \n",
      "\n",
      "I am standing in a room with a group of people.\n"
     ]
    }
   ],
   "source": [
    "result = generate_hybrid_narrative(\"/home/hipe2/Pictures/trip.jpeg\",\n",
    "                                   context=\"\")\n",
    "print(\"\\nFinal Narrative:\\n\", result[\"narrative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad5f9b-53b8-4032-947b-668a60dbf109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
